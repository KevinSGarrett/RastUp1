<!-- id: NT-0184 | source: ProjectPlans\Combined_Master_PLAIN_Non_Tech_001.docx | range: 662400-666400 -->

y_ms, model_version,
featureset_version, warnings\[\] }

SLOs

Ranking per page: p95 \< 50 ms (excluding feature fetch); Smart Invites
batch: p95 \< 300 ms.

Search pipeline hook

Retrieve eligible organic candidates (postfilter).

Fetch features, call rank.vX.

Blend: organic rank = base_match_score × calibrated P(inquiry) ×
availability/distance priors.

Guardrails: diversity by price and newseller exposure; cap dominance; do
not reorder paid slots.

Fallback: if ML fails, use heuristic scoring (match × rep × availability
× distance).

Promotions

promo_qs.vX provides QS used in paid ordering: paid_order =
log(bid)\*w + QS\*(1-w) (w from §3.3 E).

Risk & Ops

Risk outputs route to Support/T&S queue with thresholds & playbooks
(never autoban).

H\) Logging, Monitoring & Drift

Prediction logs (ml.prediction events): model_key, version,
featureset_version, request_id, topk ids & scores, latency, ab_cell,
privacy_tier=public.

Outcome join later to compute true lift/Calib (stored in Gold).

Drift monitors: PSI/KS on key features vs training; calibration error
over time; alert if thresholds breached.

Shadow deploys: new model runs silently; compare against live in
dashboards; promote on win.

I\) Fairness, Safety & Policy Constraints

No protected attributes used or inferred.

Representation safeguards: ensure new/lowreview profiles get minimum
exposure share; cap ranking dominance by any single cohort.

Explainability: percard reasons ("Instant Book, 2h response, 95 rep,
close by") in UI sourced from features, not raw SHAP.

Humanintheloop for any negative outcome (risk flags). No automated
adverse actions (FCRA).

Paid integrity: models cannot place ineligible cards into paid slots;
eligibility & caps remain rulebased (§3.3).

NSFW policy respected in all candidate generation; ML cannot override
Safe Mode.

J\) Experimentation

AB framework integrates with semantic layer (§5.6): define primary
metrics (Inquiry→Booking), guardrails (Safety reports/1k, take rate).

Power analysis for target deltas; sequential tests with early stop
rules.

Holdouts for Promotions incrementality remain enforced.

Citybycity rollouts; quick killswitch in §4.14.2 if KPIs regress.

K\) Acceptance Criteria (mark §5.11 FINAL)

Feature Store live with documented feature groups, TTLs, and
featureset_versioning; offline/online parity monitors green.

Search Reranker integrated with guardrails; p95 latency \< 50 ms;
fallback path proven.

Smart Invites model live; measurable boost to inquiries/bookings vs
control.

Promotions QS plugged into paid ordering within caps; ROAS uplift
tracked.

Risk flags route to ops queues; no autosanctions; audit trail present.

Model Registry & Cards populated; approvals captured; reproducible
artifacts stored.

Prediction logging with outcomes join; drift & calibration monitors with
alerts.

Fairness safeguards enforced (exposure floors, dominance caps, no
protected attributes).

Killswitch & rollbacks wired via feature flags; shadow→canary→100%
playbook documented.

L\) Why this works

Improves discovery and throughput without compromising fairness or
safety.

Offline/online parity + versioned features prevents silent regressions.

Clear governance, human oversight, and quick rollback make ML a safe
accelerator, not a liability.

§5.12 --- Cost, Freshness & Access Budgets

Goal: Keep the data platform fast, predictable, and affordable. Define
freshness SLOs, error budgets, spend guardrails, and perteam access
quotas so Finance, Product, Ops, and T&S can trust the numbers without
runaway warehouse bills---or lag that breaks daily ops.

A\) Scope & Principles

Freshness first where it matters (ops dashboards, fraud/risk, inbox
hygiene). Accuracy first for finance/tax close.

Budget as a product: visible, predictable limits with selfserve requests
and audit trails (no hidden throttles).

Push computation to the edges: preaggregate hot metrics, partition
properly, cache aggressively.

Make waste obvious: top query offenders,