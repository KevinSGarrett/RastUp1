<!-- id: NT-0379 | source: ProjectPlans\Combined_Master_PLAIN_Non_Tech_001.docx | range: 1364400-1368400 -->

cells configured (5--10%); dashboards wired; invalid‑click
detection live with auto‑crediting.

Provider UX:\
□ Boost setup wizard; suggested bid range; estimated clicks; QS
improvement checklist.\
□ Reporting: impressions, clicks, CTR, CPC, inquiries, bookings, GMV,
ROAS; alerts for budget depletion and invalid clicks.

QA & Monitoring:\
□ End‑to‑end click tracking; fraud heuristics test; miscategorization
auto‑pause; city ops pricing governance.

Owners & Roles:\
• Growth PM --- product settings and UX.\
• Trust --- eligibility and abuse handling.\
• Finance --- revenue recognition; invalid‑click credits accounting.\
• City Ops --- floors and featured pricing per city.

Instrumentation & Readiness Signals:\
• CPC, CTR, QS distribution; ROAS median; invalid‑click credit %; buyer
hide/report on promoted cards.\
• Organic search KPIs unaffected: S→I, I→B, satisfaction; measure
cannibalization risk.

Top Risks & Mitigations:\
• Pay‑to‑win perception → strong labeling, relevance match, caps;
eligibility bars.\
• Fraud → auto‑credits; throttle offenders; anomaly alerts; manual
review for patterns.

Acceptance Criteria (Go/No‑Go):\
✓ Holdout ROAS ≥ 2.5× median; invalid‑click credits \< 7%; organic KPIs
stable within guardrails.\
✓ Provider setup and reporting successful in dry run; billing via wallet
correct.

Scenario Playbook --- Invalid clicks surge:\
Trigger: Invalid rate \> upper bound\
Response: Tighten fraud filters; auto‑credit; temporarily reduce
density; notify affected providers.\
Roll‑back/Forward: Relax once normal; publish learning.\
Owner: Growth/Trust \| Telemetry: invalid_click_rate, ROAS \| User Copy:
"We issued credits for suspicious clicks while we improve
detection---thanks for your patience."

Scenario Playbook --- Provider backlash on labeling:\
Trigger: Feedback cites confusion or distrust\
Response: Improve \'Promoted\' tooltip; publish help page; survey to
validate clarity.\
Roll‑back/Forward: A/B previous label copy if metrics regress.\
Owner: Growth PM \| Telemetry: buyer_hide_rate, help_center_views \|
User Copy: "Promoted results match your filters and are clearly marked
to keep search fair."

Implementation Notes:\
• Flags & City Gates: Every user‑visible change ships behind a flag with
a documented blast radius, an owner, and a rollback plan. City
allowlists prevent accidental national exposure. Admin has kill‑switches
for money and safety features with two‑person approval.\
• Admin Operability: No checklist is \'done\' unless Support can resolve
the top 3 edge cases without engineering and Trust can perform required
actions with an audit trail.\
• Help‑Center Alignment: All user‑facing copy is mirrored in the help
center with last‑updated dates and visual examples. We avoid legal
jargon; we explain outcomes and next steps.\
• QA Strategy: We prefer production‑like staging with seeded data and
synthetic bookings. We test under simulated load, outage, and edge cases
(ACH returns, 3DS failures, moderation queue spikes).\
• Holdouts & Experiments: Promotions, ranking, and any economic nudges
run with control cells and pre‑registered guardrails. We pause
experiments when guardrails breach.\
• Close‑the‑Loop: Post‑launch, we hold 7/30/90‑day reviews that tie
metrics to actions and retire ineffective steps.

Implementation Notes:\
• Flags & City Gates: Every user‑visible change ships behind a flag with
a documented blast radius, an owner, and a rollback plan. City
allowlists prevent accidental national exposure. Admin has kill‑switches
for money and safety features with two‑person approval.\
• Admin Operability: No checklist is \'done\' unless Support can resolve
the top 3 edge cases without engineering and Trust can perform required
actions with an audit trail.\
• Help‑Center Alignment: All user‑facing copy is mirrored in the help
center with last‑updated dates and visual examples. We avoid legal
jargon; we explain outcomes and next steps.\
• QA Strategy: We prefer production‑like staging with seeded data