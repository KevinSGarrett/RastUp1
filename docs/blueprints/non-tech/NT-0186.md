<!-- id: NT-0186 | source: ProjectPlans\Combined_Master_PLAIN_Non_Tech_001.docx | range: 669600-673600 -->

ta for 2 hours with justification;
autoapprove within set guardrails.

J\) Telemetry (cost & freshness)

cost.warehouse.credits_used • cost.tb_scanned • cost.object_storage_gb •
cost.egress_gb

freshness.table.p95_minutes (by table) • freshness.deadline.breach

budget.team.usage • budget.team.threshold_hit (70/90/100)

query.kill (reason, tb_estimate) • query.warn (missing_partition)

mv.hit/miss • cache.hit/miss • mv.autoprune

error_budget.consume/reset (product, minutes)

K\) Acceptance Criteria (mark §5.12 FINAL)

Freshness SLOs and weekly error budgets configured; breaches consume
budget and page owners appropriately.

Cost centers with caps & alerts live; throttles wired (compute class
preemption, query kill rules).

Perteam TB/day quotas and concurrency limits enforced; "budget bar"
visible in query UI.

Safe query rules (mandatory partitions, result caps, vault blocks)
enforced with friendly preflight.

Compute classes (S1--S4) scheduled; backfills run offpeak with autopause
under S1/S2 pressure.

Materializations & caches in place; unused MVs autopruned; approx
queries available for exploration.

Admin dashboards show spend, freshness, and budget burndown; top
offenders and EXPLAIN links visible.

All telemetry events emitted; runbooks linked from alerts; budget change
approvals audited.

L\) Why this works

Predictable freshness for ops and truth at close for Finance---without
carteblanche spending.

Teams get freedom within limits and clear feedback loops when they
approach those limits.

Cost optimizations (partitions, MVs, caches) are built into the UX, not
bolted on.

Error budgets and preemption keep us stable during spikes while
preserving critical flows.

§5.13 --- Experimentation & Causal Measurement

Goal: Ship changes with proof. Run trustworthy A/B tests that respect
marketplace interference, keep trust/safety guardrails, and tie results
to the versioned Metrics Catalog (§5.6). Support both userlevel and
marketlevel experiments (e.g., fees, ranking), plus holdouts for paid
promotions (§3.3).

A\) Scope & Objectives

One platform for A/B, switchback (timebased), geo/city tests, and
alwayson holdouts.

Sticky assignment, auditable exposure logs, power analysis, sequential
monitoring, and preregistration of hypotheses/metrics.

Guardrails for safety, cancellations, takerate, and
fairness---autorollback on breach.

All results computed via the semantic layer (§5.6) with definition
hashes.

B\) Randomization Units & Assignment

Units (choose per test):

Buyerlevel (buyer_user_id) --- search/reranker UI, messaging nudges,
checkout copy.

Provider/profilelevel (service_profile_id) --- seller tools, pricing
hints, availability UI.

Thread/leglevel (message_thread_id / booking_leg_id) --- messaging
flows, deliverables UI.

Citylevel / Switchback (time slots by city) --- marketplace policies,
fee displays, highinterference features.

Campaignlevel (promotion_campaign_id) --- promotions QS/bidding
experiments.

Assignment service (deterministic & auditable):

Hash(unit_id + experiment_key + salt) → bucket in \[0..9999\]; map to
variants by allocation (e.g., 90/10).

Stratification options: by city_id, role, and traffic band to balance.

Stickiness: stored in exp.assignment table; crossdevice via account id.

Collisions: mutualexclusion sets for tests touching the same surface;
scheduler enforces only one live per surface/city.

C\) Exposure & Event Logging

Assignment: exp.assign (unit_id, experiment_key, variant, hash, time,
city, role).

Exposure: exp.expose when the unit actually sees the decision point
(e.g., results page rendered, DM nudge shown).

Primary conversions (via §5.6 metrics):

Buyer: Search→Inquiry, Inquiry→Booking, GMV, Buyer fee takerate effect.

Provider: Acceptance rate, Completion, Net earnings, Response time.

All experiment events land in Silver and Gold (gold.fact_experiment)
with joins to outcomes.

D\) Test Types & Recommended Designs

Standard A/B (low interference)

Buyerlevel for ranking