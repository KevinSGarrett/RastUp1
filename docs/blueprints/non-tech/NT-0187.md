<!-- id: NT-0187 | source: ProjectPlans\Combined_Master_PLAIN_Non_Tech_001.docx | range: 673200-677200 -->

e rendered, DM nudge shown).

Primary conversions (via §5.6 metrics):

Buyer: Search→Inquiry, Inquiry→Booking, GMV, Buyer fee takerate effect.

Provider: Acceptance rate, Completion, Net earnings, Response time.

All experiment events land in Silver and Gold (gold.fact_experiment)
with joins to outcomes.

D\) Test Types & Recommended Designs

Standard A/B (low interference)

Buyerlevel for ranking tweaks, copy, forms.

Providerlevel for dashboard UX, analytics, or hints.

Analysis: clusterrobust SEs at unit; CUPED (see §F).

Switchback (timebased by city)

Use for marketplacewide knobs (caps, density, InstantBook incentives)
where crossunit interference is strong.

Alternate treatment/control in randomized time slots (e.g., 2hour
blocks) per city; ensure stationarity checks.

Geo / City Experiments

Use distinct city cohorts as arms; match on historical KPIs;
differenceindifferences (DiD) with preperiod.

Promotion Holdouts (alwayson)

5--10% holdout of eligible impressions per city/role to estimate
incremental bookings/GMV for paid Boost/Featured (§3.3).

Sequential Rollouts (canary → city cohort → 100%)

Guardrails checked at each step; autopause on breach.

E\) PreRegistration & Guardrails

Prereg template (per test): hypothesis, units, allocation, primary
metric(s), acceptable risk, guardrails, MDE, min sample size, analysis
plan, stop rules.

Guardrails (monitored continuously):

Safety: reports per 1k legs (cityday) must not rise \> +0.2 absolute;
noshow rate must not rise \> +0.5 p.p.

Economics: takerate must not fall \> 1.0 p.p. unless explicitly allowed;
negative net booking revenue prohibited.

Fairness/Exposure: maintain minimum exposure to new providers (≥ target
share) and price diversity.

Freshness: ops data lag must meet §5.12 SLOs; otherwise, evaluation
pauses.

Autorollback: Feature flag flips to control if any guardrail crosses
threshold for N consecutive checks (configurable; default 3).

F\) Power, Estimation & Stats

Power & sample size

Builtin calculator: baseline rate, MDE, alpha, power → sample per
variant.

Use clustered power calcs when units generate multiple events (e.g.,
buyer searches many times).

Estimators

Differenceinmeans with clusterrobust SEs (unitclustered).

CUPED for variance reduction using preperiod covariates (e.g., buyer's
historical conversion).

DiD for city/switchback (pre vs post × treat vs control).

Winsorization for heavytailed \$\$ outcomes; or use medianofmeans.

Sequential tests: O'BrienFleming or Pocock boundaries for early stopping
with overall α control.

Multiple testing

Limit simultaneous tests per surface; apply BenjaminiHochberg FDR for
families of related metrics.

G\) Analysis Pipeline & Reporting

Analysis runs from Gold via the semantic layer (§5.6) to ensure correct
grains and LBG handling.

Each test produces an experiment report artifact: variant effects, CIs,
pvalues, CUPED details, slice table (city, role, rep decile, price
decile), guardrail plots, and definition hashes for all metrics used.

Reports stored with exp_report_id and linked in Admin → Experiments.

H\) Interference, Spillovers & Bias Controls

Buyer crossover: if a buyer in variant A messages a provider in variant
B, the buyer's assignment governs buyerside outcomes; leglevel outcomes
are attributed using the decision path (documented).

Provider capacity effects: for ranking tests, cap exposure so one
variant doesn't monopolize top slots (diversity constraints).

Time trends: switchback blocks randomized; include timeofday/dayofweek
fixed effects.

Simultaneous tests: mutualexclusion sets; RFC needed to corun on same
surface.

I\) Ethics & Policy

No protected attributes used for assignment or modeling.

Risk models flag only; no automated adverse actions (§5.11).

NSFW policy remains enforced in candidates; experiments cannot bypass
Safe Mode.

Informed UX: no "dark patterns" (e.g., fake scarcity); experiments may
vary copy but not materially mislead.

Data use: limited to platform impro